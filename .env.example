# AI Docker Offload Configuration

# Service URLs (for local development)
TRITON_URL=triton-server:8000
PREPROCESSOR_URL=preprocessor:8000
COORDINATOR_URL=inference-coordinator:8080
AGGREGATOR_URL=aggregator:8000

# GPU Configuration
NVIDIA_VISIBLE_DEVICES=all
GPU_MEMORY_LIMIT=8Gi
CUDA_VISIBLE_DEVICES=0

# Docker Offload Configuration
OFFLOAD_ENABLED=false
CLOUD_ENDPOINT=
EDGE_ENDPOINT=
OFFLOAD_PRIORITY=gpu-required

# Resource Limits
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT=30
BATCH_SIZE=4
MODEL_CACHE_SIZE=2Gi

# Logging and Monitoring
LOG_LEVEL=INFO
METRICS_ENABLED=true
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000

# Security
API_KEY=
ENABLE_TLS=false
CERT_PATH=
KEY_PATH=

# Performance Tuning
WORKER_PROCESSES=4
THREAD_POOL_SIZE=8
MODEL_WARMUP=true
ENABLE_BATCHING=true

# Development/Testing
MOCK_MODELS=true
DEBUG_MODE=false
TEST_DATA_PATH=./test-data